Using AutoGluon’s Tabular Prediction, I ran multiple experiments—an initial raw model, a feature-engineered model, and three hyperparameter-optimized (HPO) variants—discovering that negative predictions caused Kaggle rejections and therefore replacing them with zero; the feature-engineered “WeightedEnsemble_L3” model emerged as best, achieving a validation RMSE of 37.98 and a Kaggle score of 0.44798 without HPO. Performance gains (≈138 % over the raw baseline) came from parsing datetime into year, month, day-of-week, and hour; recasting season and weather as categorical; creating a day_type (weekday / weekend / holiday); and dropping highly correlated (atemp) or train-only (casual, registered) features to reduce multicollinearity. Three HPO runs—using lighter presets such as “optimize_for_deployment” to stay within memory and time limits—yielded competitive but inferior Kaggle scores, highlighting AutoGluon’s exploration-vs-exploitation trade-off under constrained hyperparameter grids. Given more time and resources, extended runs with high-quality presets and broader hyperparameter ranges could further improve results, but this project ultimately showed that thoughtful exploratory data analysis and feature engineering, coupled with AutoGluon’s automated ensembling, delivered the strongest gains for predicting Capital Bikeshare demand.